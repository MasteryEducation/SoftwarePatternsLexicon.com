---
linkTitle: "Calibration Plots"
title: "Calibration Plots: Comparing Predicted Probabilities to Actual Outcomes for Calibration Assessment"
description: "Long Description"
categories:
- Model Validation and Evaluation Patterns
- Advanced Evaluation Techniques
tags:
- Calibration Plots
- Probability Calibration
- Model Evaluation
- Machine Learning
- Model Calibration
date: 2023-10-14
type: docs
canonical: "https://softwarepatternslexicon.com/machine-learning/model-validation-and-evaluation-patterns/advanced-evaluation-techniques/calibration-plots"
license: "© 2024 Tokenizer Inc. CC BY-NC-SA 4.0"
---


Calibration plots, also known as reliability diagrams, are crucial tools for assessing the quality of probabilistic predictions. These plots compare the predicted probabilities generated by a model to the actual outcomes observed in the dataset, providing insight into whether the probabilities are well-calibrated.

Calibration is a measure of the agreement between predicted probability and the actual occurrence of events. A well-calibrated model will output probabilities that are reflective of the true likelihood of outcomes. For example, if a model predicts a 70% probability of the positive class, in a perfectly calibrated model, roughly 70 out of 100 predicted instances should belong to the positive class.

## Understanding Calibration Plots

A calibration plot typically displays:
- **The x-axis**: Representing predicted probabilities divided into equal intervals, or bins.
- **The y-axis**: Actually observed frequency of the positive class within those bins.

A perfectly calibrated model means that for the bin corresponding to a predicted probability of 0.1, around 10% of the instances should actually belong to the positive class, for the bin of predicted probability 0.2, around 20% should belong to the positive class, and so on.

The key insights you derive from the plot:
- **Over-confidence**: The predicted probabilities are higher than the actual outcomes.
- **Under-confidence**: The predicted probabilities are lower than the actual outcomes.
- **Well-calibrated**: Points lie along the diagonal line $y = x$.

## Creating Calibration Plots

### Calibration Diagram Construction Steps
1. **Predict Probabilities**: Use your model to predict probabilities on a validation set.
2. **Bin Predictions**: Divide the predicted probabilities into bins.
3. **Calculate Frequencies**: For each bin, calculate the frequency of the positive class.
4. **Plot the Calibration Curve**: Plot the observed frequencies against the predicted probabilities.

## Examples

### Python with scikit-learn

Python provides multiple libraries to create a calibration plot straightforwardly. Below we use `scikit-learn`:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

X, y  = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)
prob_pos = model.predict_proba(X_test)[:, 1]

prob_true, prob_pred = calibration_curve(y_test, prob_pos, n_bins=10)

plt.figure(figsize=(10, 8))
plt.plot(prob_pred, prob_true, marker='o', linewidth=1, label='Logistic Regression')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.title('Calibration Plot')
plt.legend()
plt.show()
```

### R with caret

In R, the `caret` package simplifies steps required to generate a calibration plot:

```R
library(caret)
library(ggplot2)

set.seed(42)
train_data <- twoClassSim(1000)
test_data <- twoClassSim(400)

model <- train(Class ~ ., data=train_data, method="glm", family="binomial")

pred <- predict(model, test_data, type='prob')

cal <- calibration(Class ~ pred[,2], data=test_data, class='Class2')

ggplot(cal, aes(x=midpoint, y=Percent)) + 
    geom_line() + 
    geom_point(aes(color='data')) + 
    geom_abline(slope=1, intercept=0, linetype="dashed", color="blue") +
    labs(x = "Mean Predicted Probability", y = "Fraction of Positives",
         title = "Calibration Plot") + 
    theme_minimal()
```

## Related Design Patterns

- **Confusion Matrix**: Provides deeper insights into the performance of your classification model by showing the actual vs predicted outcomes in a matrix format.
- **ROC Curves**: Complement the calibration plots by depicting the true positive rate against the false positive rate for better threshold selection and model evaluation.
- **Precision-Recall Curves**: Also integral for evaluating classification performance, especially in datasets with imbalanced classes.

## Additional Resources

- "Introduction to Machine Learning with Python" by Andreas C. Müller and Sarah Guido.
- "Machine Learning Yearning" by Andrew Ng.
- Scikit-learn documentation on [calibration curves](https://scikit-learn.org/stable/modules/calibration.html).

## Summary

Calibration plots are essential tools in the arsenal of model evaluation techniques, especially for models that output probabilities. They allow us to understand and visualize the degree to which predicted probabilities correspond to actual outcome frequencies. By focusing on calibration, we can improve the reliability and interpretability of probabilistic predictions, a crucial factor in many real-world applications such as medical diagnosis, risk assessment and decision-making processes in uncertain environments.

Ensuring your model is well-calibrated isn't just about improving your metrics but is also crucial for generating trustworthy predictions that stakeholders can rely on.
